{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T16:21:22.151146Z",
     "start_time": "2018-04-14T16:21:22.147886Z"
    }
   },
   "source": [
    "## LECTURA DE FICHEROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T19:31:23.270909Z",
     "start_time": "2018-04-15T19:29:02.065Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## INFO DE PROYECTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T19:31:23.272267Z",
     "start_time": "2018-04-15T19:29:29.568Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### NOTAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T19:31:23.269856Z",
     "start_time": "2018-04-15T19:31:23.184534Z"
    },
    "hidden": true
   },
   "source": [
    "churn -> 30 DÍAS sin contrato\n",
    "\n",
    "\n",
    "Clientes especiales: \n",
    "* TODO 0 -> YyO+tlZtAXYXoZhNr3Vg3+dfVQvrBVGO8j1mfqe4ZHc="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T16:16:57.245541Z",
     "start_time": "2018-04-14T16:16:57.240789Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## RUTAS DE CONFIGURACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T09:17:09.028454Z",
     "start_time": "2018-04-15T09:17:09.022785Z"
    },
    "deletable": false,
    "editable": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "RUTA_STORAGE = \"kkboxschurn/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T09:17:10.329384Z",
     "start_time": "2018-04-15T09:17:10.317477Z"
    },
    "deletable": false,
    "editable": false,
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f5f5bbc9f037>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspark_session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_spark_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f5f5bbc9f037>\u001b[0m in \u001b[0;36mcreate_spark_session\u001b[0;34m(app_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_spark_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Modelo de propension\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mspark_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_name\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mspark_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLogLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WARN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.sql.crossJoin.enabled\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "def create_spark_session(app_name=\"Modelo de propension\"):\n",
    "    spark_session = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .getOrCreate()\n",
    "            \n",
    "    spark_session.sparkContext.setLogLevel(\"WARN\")\n",
    "    spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "       \n",
    "    return spark_session\n",
    "\n",
    "session = create_spark_session()\n",
    "sc = session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## IMPORTACIÓN DE LIBRERÍAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T09:17:13.947007Z",
     "start_time": "2018-04-15T09:17:13.645611Z"
    },
    "deletable": false,
    "editable": false,
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-96c3179e959d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    " #! pip install datalab\\n\"\n",
    "import json\n",
    "import pprint\n",
    "import subprocess\n",
    "import pyspark\n",
    "import time\n",
    "from pyspark.sql import SQLContext,DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import udf, array, col, explode, lit, struct, rank, desc,to_date, add_months, row_number, isnan, isnull, when, count\n",
    "from pyspark.sql.types import IntegerType, DoubleType ,Row , StructType\n",
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from typing import Iterable \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce  \n",
    "#import datalab.storage as gcs\\n\",\n",
    "from pyspark.storagelevel import StorageLevel \n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T16:12:14.737794Z",
     "start_time": "2018-04-14T16:12:14.730894Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## TRANSFORMACIÓN DE FICHEROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T19:31:28.232284Z",
     "start_time": "2018-04-15T19:31:28.227157Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Esto es para pasar de csv a parquet (ficheros distribuidos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T16:36:42.894635Z",
     "start_time": "2018-04-14T16:24:23.387126Z"
    },
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3db84ede1dbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gs://\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRUTA_STORAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"input/transactions.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtransactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gs://\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRUTA_STORAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"input_tratado/transactions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmembers_v3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gs://\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRUTA_STORAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"input/members_v3.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmembers_v3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gs://\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRUTA_STORAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"input_tratado/members_v3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "transactions = spark.read.csv(\"gs://\"+str(RUTA_STORAGE)+\"input/transactions.csv\", header=True)\n",
    "transactions.write.format(\"parquet\").save(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/transactions\")\n",
    "\n",
    "members_v3 = spark.read.csv(\"gs://\"+str(RUTA_STORAGE)+\"input/members_v3.csv\", header=True)\n",
    "members_v3.write.format(\"parquet\").save(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/members_v3\")\n",
    "\n",
    "train_v2 = spark.read.csv(\"gs://\"+str(RUTA_STORAGE)+\"input/train_v2.csv\", header=True)\n",
    "train_v2.write.format(\"parquet\").save(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/train_v2\")\n",
    "\n",
    "train = spark.read.csv(\"gs://\"+str(RUTA_STORAGE)+\"input/train.csv\", header=True)\n",
    "train.write.format(\"parquet\").save(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/train\")\n",
    "\n",
    "transactions_v2 = spark.read.csv(\"gs://\"+str(RUTA_STORAGE)+\"input/transactions_v2.csv\", header=True)\n",
    "transactions_v2.write.format(\"parquet\").save(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/transactions_v2\")\n",
    "\n",
    "user_logs = spark.read.csv(\"gs://\"+str(RUTA_STORAGE)+\"input/user_logs.csv\", header=True)\n",
    "user_logs.write.format(\"parquet\").save(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/user_logs\")\n",
    "\n",
    "user_logs_v2 = spark.read.csv(\"gs://\"+str(RUTA_STORAGE)+\"input/user_logs_v2.csv\", header=True)\n",
    "user_logs_v2.write.format(\"parquet\").save(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/user_logs_v2\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T19:31:23.157815Z",
     "start_time": "2018-04-15T19:31:23.149494Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Leemos los ficheros almacenados en .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T09:18:00.794947Z",
     "start_time": "2018-04-15T09:17:31.837047Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "transactions = sqlContext.read.parquet(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/transactions\")\n",
    "transactions.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "members_v3 = sqlContext.read.parquet(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/members_v3\")\n",
    "members_v3.createOrReplaceTempView(\"members_v3\")\n",
    "\n",
    "train_v2 = sqlContext.read.parquet(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/train_v2\")\n",
    "train_v2.createOrReplaceTempView(\"train_v2\")\n",
    "\n",
    "train = sqlContext.read.parquet(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/train\")\n",
    "train.createOrReplaceTempView(\"train\")\n",
    "\n",
    "transactions_v2 = sqlContext.read.parquet(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/transactions_v2\")\n",
    "transactions_v2.createOrReplaceTempView(\"transactions_v2\")\n",
    "\n",
    "user_logs_v2 = sqlContext.read.parquet(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/user_logs_v2\")\n",
    "user_logs_v2.createOrReplaceTempView(\"user_logs_v2\")\n",
    "\n",
    "user_logs = sqlContext.read.parquet(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/user_logs\")\n",
    "user_logs.createOrReplaceTempView(\"user_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T19:31:23.165921Z",
     "start_time": "2018-04-15T19:31:23.159842Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Unimos los ficheros log y log2   y transacciones y transacciones2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T09:57:53.341977Z",
     "start_time": "2018-04-15T09:57:53.263681Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "total_elementos_log = sqlContext.sql(\"select msno,date,num_25,num_50,num_75,num_985,num_100,num_unq,total_secs, \\\n",
    "                                             from_unixtime(unix_timestamp(date, 'yyyyMMdd')) as date_t \\\n",
    "                                      from user_logs \\\n",
    "                                      union all  \\\n",
    "                                      select msno,date,num_25,num_50,num_75,num_985,num_100,num_unq,total_secs, \\\n",
    "                                      from_unixtime(unix_timestamp(date, 'yyyyMMdd')) as date_t \\\n",
    "                                      from user_logs_v2\")\n",
    "\n",
    "total_elementos_transactions = sqlContext.sql(\"select msno,payment_method_id,payment_plan_days,plan_list_price,actual_amount_paid,is_auto_renew,transaction_date,membership_expire_date, \\\n",
    "                       from_unixtime(unix_timestamp(transaction_date, 'yyyyMMdd')) as transaction_date_t, \\\n",
    "                       date_sub(from_unixtime(unix_timestamp(membership_expire_date, 'yyyyMMdd')),1) as membership_expire_date_t \\\n",
    "                from transactions \\\n",
    "                union all \\\n",
    "                select msno,payment_method_id,payment_plan_days,plan_list_price,actual_amount_paid,is_auto_renew,transaction_date,membership_expire_date, \\\n",
    "                       from_unixtime(unix_timestamp(transaction_date, 'yyyyMMdd')) as transaction_date_t, \\\n",
    "                       date_sub(from_unixtime(unix_timestamp(membership_expire_date, 'yyyyMMdd')),1) as membership_expire_date_t \\\n",
    "                from transactions_v2 \\\n",
    "                \")\n",
    "\n",
    "total_elementos_log.cache()\n",
    "total_elementos_transactions.cache()\n",
    "\n",
    "total_elementos_log.createOrReplaceTempView(\"total_elementos_log\")\n",
    "total_elementos_transactions.createOrReplaceTempView(\"total_elementos_transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T19:31:23.171081Z",
     "start_time": "2018-04-15T19:31:23.168495Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Relacionamos todos los elementos del log con cada transacción \n",
    "TODO:(Solucionar el tema de fechas solapadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T13:48:15.092926Z",
     "start_time": "2018-04-15T13:02:28.141696Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pre_salida = sqlContext.sql(\"select t.msno as msno ,transaction_date,membership_expire_date,membership_expire_date_t , \\\n",
    "                        payment_method_id,payment_plan_days,plan_list_price,actual_amount_paid, \\\n",
    "                        is_auto_renew,  \\\n",
    "                        date,num_25,num_50,num_75,num_985,num_100,num_unq,total_secs  \\\n",
    "                from total_elementos_transactions t \\\n",
    "                left join total_elementos_log l on \\\n",
    "                l.msno = t.msno and transaction_date_t <= date_t and  membership_expire_date_t >= date_t  \")\n",
    "\n",
    "\n",
    "pre_salida.write.format(\"parquet\").save(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/pre_salida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T19:31:23.175863Z",
     "start_time": "2018-04-15T19:31:23.173078Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Eliminamos las transacciones que no tienen ninguna operación en el log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T14:14:46.193754Z",
     "start_time": "2018-04-15T14:14:46.144854Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pre_salida = sqlContext.read.parquet(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/pre_salida\")\n",
    "pre_salida.createOrReplaceTempView(\"pre_salida\")\n",
    "\n",
    "pre_salida_filtrada =  sqlContext.sql(\"select msno,transaction_date,membership_expire_date, \\\n",
    "                 from_unixtime(unix_timestamp(membership_expire_date_t, 'yyyy-MM-dd')) as membership_expire_date_t, \\\n",
    "                 payment_method_id,payment_plan_days,plan_list_price,actual_amount_paid,is_auto_renew, \\\n",
    "                 from_unixtime(unix_timestamp(date, 'yyyyMMdd')) as date, \\\n",
    "                 num_25,num_50,num_75,num_985,num_100,num_unq,total_secs \\\n",
    "                       from pre_salida where date is not null\")\n",
    "pre_salida_filtrada.cache()\n",
    "pre_salida_filtrada.createOrReplaceTempView(\"pre_salida_filtrada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Creamos las variables a nivel de transacción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T15:33:24.959055Z",
     "start_time": "2018-04-15T14:34:09.684725Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions  import date_format\n",
    "\n",
    "salida = sqlContext.sql(\"select msno, transaction_date,membership_expire_date, \\\n",
    "                    count(*) as dias_contratados,\\\n",
    "                    sum(IF(total_secs >0 ,1,0)) as dias_conexion,\\\n",
    "                    sum(IF( num_25 >0 ,1,0)) as num_25,\\\n",
    "                    sum(IF( num_50 >0 ,1,0)) as num_50,\\\n",
    "                    sum(IF( num_75 >0 ,1,0)) as num_75,\\\n",
    "                    sum(IF( num_985 >0 ,1,0)) as num_985,\\\n",
    "                    sum(IF( num_100 >0 ,1,0)) as num_100,\\\n",
    "                    sum(IF( num_unq >0 ,1,0)) as num_unq,\\\n",
    "                    sum(IF(total_secs >0 ,1,0)) as total_secs,\\\n",
    "                    \\\n",
    "                    sum(IF(date = s.membership_expire_date_t and total_secs >0 ,1,0)) as dias_conexion_ld,\\\n",
    "                    sum(IF(date = s.membership_expire_date_t and num_25 >0 ,1,0)) as num_25_ld,\\\n",
    "                    sum(IF(date = s.membership_expire_date_t and num_50 >0 ,1,0)) as num_50_ld,\\\n",
    "                    sum(IF(date = s.membership_expire_date_t and num_75 >0 ,1,0)) as num_75_ld,\\\n",
    "                    sum(IF(date = s.membership_expire_date_t and num_985 >0 ,1,0)) as num_985_ld,\\\n",
    "                    sum(IF(date = s.membership_expire_date_t and num_100 >0 ,1,0)) as num_100_ld,\\\n",
    "                    sum(IF(date = s.membership_expire_date_t and num_unq >0 ,1,0)) as num_unq_ld,\\\n",
    "                    sum(IF(date = s.membership_expire_date_t and total_secs >0 ,1,0)) as total_secs_ld, \\\n",
    "                \\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,7) and total_secs >0 ,1,0)) as dias_conexion_l7d,\\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,7) and num_25 >0 ,1,0)) as num_25_l7d,\\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,7) and num_50 >0 ,1,0)) as num_50_l7d,\\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,7) and num_75 >0 ,1,0)) as num_75_l7d,\\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,7) and num_985 >0 ,1,0)) as num_985_l7d,\\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,7) and num_100 >0 ,1,0)) as num_100_l7d,\\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,7) and num_unq >0 ,1,0)) as num_unq_l7d,\\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,7) and total_secs >0 ,1,0)) as total_secs_l7d, \\\n",
    "                    \\\n",
    "                     \\\n",
    "                    sum(IF(date  >= date_sub(s.membership_expire_date_t,28) and total_secs >0 ,1,0)) as dias_conexion_l28d,\\\n",
    "                    sum(IF(date  >= date_sub(s.membership_expire_date_t,28) and num_25 >0 ,1,0)) as num_25_l28d,\\\n",
    "                    sum(IF(date  >= date_sub(s.membership_expire_date_t,28) and num_50 >0 ,1,0)) as num_50_l28d,\\\n",
    "                    sum(IF(date  >= date_sub(s.membership_expire_date_t,28) and num_75 >0 ,1,0)) as num_75_l28d,\\\n",
    "                    sum(IF(date  >= date_sub(s.membership_expire_date_t,28) and num_985 >0 ,1,0)) as num_985_l28d,\\\n",
    "                    sum(IF(date  >= date_sub(s.membership_expire_date_t,28) and num_100 >0 ,1,0)) as num_100_l28d,\\\n",
    "                    sum(IF(date  >= date_sub(s.membership_expire_date_t,28) and num_unq >0 ,1,0)) as num_unq_l28d,\\\n",
    "                    sum(IF(date  >= date_sub(s.membership_expire_date_t,28) and total_secs >0 ,1,0)) as total_secs_l28d, \\\n",
    "                    \\\n",
    "                    sum(num_25) as num_25v,\\\n",
    "                    sum(num_50) as num_50v,\\\n",
    "                    sum(num_75) as num_75v,\\\n",
    "                    sum(num_985) as num_985v,\\\n",
    "                    sum(num_100) as num_100v,\\\n",
    "                    sum(num_unq) as num_unqv,\\\n",
    "                    sum(total_secs) as total_secsv,\\\n",
    "                    \\\n",
    "                    sum(IF(date = s.membership_expire_date_t and num_25 >0 ,num_25,0)) as num_25_ldv,\\\n",
    "                    sum(IF(date = s.membership_expire_date_t and num_50 >0 ,num_50,0)) as num_50_ldv,\\\n",
    "                    sum(IF(date = s.membership_expire_date_t and num_75 >0 ,num_75,0)) as num_75_ldv,\\\n",
    "                    sum(IF(date = s.membership_expire_date_t and num_985 >0 ,num_985,0)) as num_985_ldv,\\\n",
    "                    sum(IF(date = s.membership_expire_date_t and num_100 >0 ,num_100,0)) as num_100_ldv,\\\n",
    "                    sum(IF(date = s.membership_expire_date_t and num_unq >0 ,num_unq,0)) as num_unq_ldv,\\\n",
    "                    sum(IF(date = s.membership_expire_date_t and total_secs >0 ,total_secs,0)) as total_secs_ldv, \\\n",
    "                \\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,7) and num_25 >0 ,num_25,0)) as num_25_l7dv,\\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,7) and num_50 >0 ,num_50,0)) as num_50_l7dv,\\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,7) and num_75 >0 ,num_75,0)) as num_75_l7dv,\\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,7) and num_985 >0 ,num_985,0)) as num_985_l7dv,\\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,7) and num_100 >0 ,num_100,0)) as num_100_l7dv,\\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,7) and num_unq >0 ,num_unq,0)) as num_unq_l7dv,\\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,7) and total_secs >0 ,total_secs,0)) as total_secs_l7dv, \\\n",
    "                    \\\n",
    "                     \\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,28) and num_25 >0 ,num_25,0)) as num_25_l28dv,\\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,28) and num_50 >0 ,num_50,0)) as num_50_l28dv,\\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,28) and num_75 >0 ,num_75,0)) as num_75_l28dv,\\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,28) and num_985 >0 ,num_985,0)) as num_985_l28dv,\\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,28) and num_100 >0 ,num_100,0)) as num_100_l28dv,\\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,28) and num_unq >0 ,num_unq,0)) as num_unq_l28dv,\\\n",
    "                    sum(IF(date >= date_sub(s.membership_expire_date_t,28) and total_secs >0 ,total_secs,0)) as total_secs_l28dv, \\\n",
    "                    \\\n",
    "                     max(IF(date <= s.membership_expire_date and total_secs >0 ,date,0)) as ultimos_dias_sin_conectarte\\\n",
    "                \\\n",
    "                from pre_salida_filtrada s \\\n",
    "               group by msno , transaction_date,membership_expire_date \\\n",
    "               \")\n",
    "\n",
    "salida.write.format(\"parquet\").save(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/elementos_modelo4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Creamos las variables a nivel de usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T19:09:55.844837Z",
     "start_time": "2018-04-15T18:05:09.071563Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "elementos_modelo4 = sqlContext.read.parquet(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/elementos_modelo4\")\n",
    "elementos_modelo4.createOrReplaceTempView(\"elementos_modelo4\")\n",
    "\n",
    "em = sqlContext.sql(\"select city,bd,gender,registered_via,registration_init_time,em.*, \\\n",
    "                     dense_rank() OVER (PARTITION BY em.msno ORDER BY transaction_date ) as num_suscr, \\\n",
    "                     datediff(from_unixtime(unix_timestamp(lead(transaction_date,1) OVER (PARTITION BY em.msno ORDER BY transaction_date  ), 'yyyyMMdd')),from_unixtime(unix_timestamp(membership_expire_date, 'yyyyMMdd'))) as sig_sub, \\\n",
    "                     sum(dias_contratados) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as dias_contratados_acumulado,\\\n",
    "                     sum(dias_conexion) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as dias_conexion_acumulado,\\\n",
    "                     sum(num_25) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_25_acumulado,\\\n",
    "                     sum(num_50) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_50_acumulado,\\\n",
    "                     sum(num_75) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_75_acumulado,\\\n",
    "                     sum(num_985) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_985_acumulado,\\\n",
    "                     sum(num_100) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_100_acumulado,\\\n",
    "                     sum(num_unq) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_unq_acumulado,\\\n",
    "                     sum(total_secs) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as total_secs_acumulado,\\\n",
    "                     sum(dias_conexion_ld) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as dias_conexion_ld_acumulado,\\\n",
    "                     sum(num_25_ld) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_25_ld_acumulado,\\\n",
    "                     sum(num_50_ld) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_50_ld_acumulado,\\\n",
    "                     sum(num_75_ld) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_75_ld_acumulado,\\\n",
    "                     sum(num_985_ld) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_985_ld_acumulado,\\\n",
    "                     sum(num_100_ld) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_100_ld_acumulado,\\\n",
    "                     sum(num_unq_ld) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_unq_ld_acumulado,\\\n",
    "                     sum(total_secs_ld) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as total_secs_ld_acumulado,\\\n",
    "                     sum(dias_conexion_l7d) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as dias_conexion_l7d_acumulado,\\\n",
    "                     sum(num_25_l7d) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_25_l7d_acumulado,\\\n",
    "                     sum(num_50_l7d) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_50_l7d_acumulado,\\\n",
    "                     sum(num_75_l7d) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_75_l7d_acumulado,\\\n",
    "                     sum(num_985_l7d) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_985_l7d_acumulado,\\\n",
    "                     sum(num_100_l7d) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_100_l7d_acumulado,\\\n",
    "                     sum(num_unq_l7d) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_unq_l7d_acumulado,\\\n",
    "                     sum(total_secs_l7d) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as total_secs_l7d_acumulado,\\\n",
    "                     sum(dias_conexion_l28d) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as dias_conexion_l28d_acumulado,\\\n",
    "                     sum(num_25_l28d) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_25_l28d_acumulado,\\\n",
    "                     sum(num_50_l28d) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_50_l28d_acumulado,\\\n",
    "                     sum(num_75_l28d) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_75_l28d_acumulado,\\\n",
    "                     sum(num_985_l28d) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_985_l28d_acumulado,\\\n",
    "                     sum(num_100_l28d) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_100_l28d_acumulado,\\\n",
    "                     sum(num_unq_l28d) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_unq_l28d_acumulado,\\\n",
    "                     sum(total_secs_l28d) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as total_secs_l28d_acumulado,\\\n",
    "                     sum(num_25v) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_25v_acumulado,\\\n",
    "                     sum(num_50v) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_50v_acumulado,\\\n",
    "                     sum(num_75v) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_75v_acumulado,\\\n",
    "                     sum(num_985v) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_985v_acumulado,\\\n",
    "                     sum(num_100v) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_100v_acumulado,\\\n",
    "                     sum(num_unqv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_unqv_acumulado,\\\n",
    "                     sum(total_secsv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as total_secsv_acumulado,\\\n",
    "                     sum(num_25_ldv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_25_ldv_acumulado,\\\n",
    "                     sum(num_50_ldv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_50_ldv_acumulado,\\\n",
    "                     sum(num_75_ldv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_75_ldv_acumulado,\\\n",
    "                     sum(num_985_ldv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_985_ldv_acumulado,\\\n",
    "                     sum(num_100_ldv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_100_ldv_acumulado,\\\n",
    "                     sum(num_unq_ldv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_unq_ldv_acumulado,\\\n",
    "                     sum(total_secs_ldv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as total_secs_ldv_acumulado,\\\n",
    "                     sum(num_25_l7dv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_25_l7dv_acumulado,\\\n",
    "                     sum(num_50_l7dv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_50_l7dv_acumulado,\\\n",
    "                     sum(num_75_l7dv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_75_l7dv_acumulado,\\\n",
    "                     sum(num_985_l7dv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_985_l7dv_acumulado,\\\n",
    "                     sum(num_100_l7dv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_100_l7dv_acumulado,\\\n",
    "                     sum(num_unq_l7dv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_unq_l7dv_acumulado,\\\n",
    "                     sum(total_secs_l7dv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as total_secs_l7dv_acumulado,\\\n",
    "                     sum(num_25_l28dv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_25_l28dv_acumulado,\\\n",
    "                     sum(num_50_l28dv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_50_l28dv_acumulado,\\\n",
    "                     sum(num_75_l28dv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_75_l28dv_acumulado,\\\n",
    "                     sum(num_985_l28dv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_985_l28dv_acumulado,\\\n",
    "                     sum(num_100_l28dv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_100_l28dv_acumulado,\\\n",
    "                     sum(num_unq_l28dv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as num_unq_l28dv_acumulado,\\\n",
    "                     sum(total_secs_l28dv) over (PARTITION BY em.msno order by transaction_date rows unbounded preceding) as total_secs_l28dv_acumulado \\\n",
    "                     from elementos_modelo4 em \\\n",
    "                     left join members_v3 me on me.msno = em.msno \\\n",
    "                     order by em.msno, transaction_date desc\")\n",
    "\n",
    "em.write.format(\"parquet\").save(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/final_elementos_para_modelo2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T19:31:23.181213Z",
     "start_time": "2018-04-15T19:31:23.177856Z"
    }
   },
   "source": [
    "### Filtramos los elementos que no queremos en el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "final_elementos_para_modelo = sqlContext.read.parquet(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/final_elementos_para_modelo2\")\n",
    "final_elementos_para_modelo.createOrReplaceTempView(\"final_elementos_para_modelo\")\n",
    "final_elementos_para_modelo.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T19:58:49.305241Z",
     "start_time": "2018-04-15T19:58:49.302202Z"
    }
   },
   "source": [
    "### Primera prueba_ filtramos los que no estén en el fichero de members y no sean la última transacción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T19:58:49.300703Z",
     "start_time": "2018-04-15T19:57:10.052578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|gender|count(1)|\n",
      "+------+--------+\n",
      "|  null| 8011185|\n",
      "|female| 4505945|\n",
      "|  male| 4964546|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select gender, count(*) from final_elementos_para_modelo group by gender\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T20:28:29.729695Z",
     "start_time": "2018-04-15T20:28:29.656141Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "seleccionado_para_modelo = sqlContext.sql(\"select * from final_elementos_para_modelo where (sig_sub is not null or (sig_sub is null and membership_expire_date < '20170401')) and gender is not null\")\n",
    "seleccionado_para_modelo.cache()\n",
    "seleccionado_para_modelo.createOrReplaceTempView(\"seleccionado_para_modelo\")\n",
    "#df.withColumn('norm_val', (df.val-min)/(max-min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T21:57:22.175589Z",
     "start_time": "2018-04-15T21:57:22.141377Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "datos_preparados = sqlContext.sql(\"select  if (sig_sub <30,0,1) as label, array(dias_conexion/dias_contratados,if(gender == 'male',1,0)) as features  from seleccionado_para_modelo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "msno\n",
    "transaction_date\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T16:42:12.640057Z",
     "start_time": "2018-04-14T16:42:12.636871Z"
    },
    "code_folding": [],
    "heading_collapsed": true
   },
   "source": [
    "## BORRAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T16:58:04.205177Z",
     "start_time": "2018-04-14T16:58:04.171027Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "fecha_registro = sqlContext.sql(\"select msno ,min(transaction_date) as fecha_registro  from transactions group by msno\")\n",
    "fecha_registro.cache()\n",
    "fecha_registro.createOrReplaceTempView(\"fecha_registro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T17:02:02.382468Z",
     "start_time": "2018-04-14T17:01:27.915385Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "total_dias = sqlContext.sql(\"select date from user_logs  group by date order by date\")\n",
    "total_dias.cache()\n",
    "total_dias.createOrReplaceTempView(\"total_dias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T17:46:58.625076Z",
     "start_time": "2018-04-14T17:46:58.049114Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "elementos = sqlContext.sql(\"select td.date, fr.msno, \\\n",
    "                            IF(num_25 is NULL,0,num_25) as num_25,\\\n",
    "                            IF(num_50 is NULL,0,num_50) as num_50,\\\n",
    "                            IF(num_75 is NULL,0,num_75) as num_75,\\\n",
    "                            IF(num_985 is NULL,0,num_985) as num_985,\\\n",
    "                            IF(num_100 is NULL,0,num_100) as num_100,\\\n",
    "                            IF(num_unq is NULL,0,num_unq) as num_unq,\\\n",
    "                            IF(total_secs is NULL,0,total_secs) as total_secs\\\n",
    "                from total_dias td \\\n",
    "                join fecha_registro fr on fr.fecha_registro <= td.date\\\n",
    "                left join user_logs l on  l.date == td.date and l.msno == fr.msno \\\n",
    "                \\\n",
    "                \")\n",
    "elementos.createOrReplaceTempView(\"elementos\")\n",
    "elementos.write.format(\"parquet\").save(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/elementos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T22:13:31.411191Z",
     "start_time": "2018-04-14T22:13:31.317365Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "susc_filtrada =  sqlContext.sql(\"select msno,transaction_date,membership_expire_date, \\\n",
    "                        dense_rank() OVER (PARTITION BY msno ORDER BY transaction_date ) as num_suscr, \\\n",
    "                        datediff(from_unixtime(unix_timestamp(lead(transaction_date,1) OVER (PARTITION BY msno ORDER BY transaction_date  ), 'yyyyMMdd')),from_unixtime(unix_timestamp(membership_expire_date, 'yyyyMMdd'))) as sig_sub, \\\n",
    "                        date_sub(from_unixtime(unix_timestamp(transaction_date, 'yyyyMMdd')),0) as transaction_date_t, \\\n",
    "                        date_sub(from_unixtime(unix_timestamp(membership_expire_date, 'yyyyMMdd')),1) as membership_expire_date_t \\\n",
    "                from transactions t  where msno = 'DwOmWms8HZUzNPPfiRwOMHj480D+GGqkDhNnSmZJm3s=' order by msno, transaction_date\")\n",
    "\n",
    "susc_total =  sqlContext.sql(\"select msno,transaction_date,membership_expire_date, \\\n",
    "                        dense_rank() OVER (PARTITION BY msno ORDER BY transaction_date ) as num_suscr, \\\n",
    "                        datediff(from_unixtime(unix_timestamp(lead(transaction_date,1) OVER (PARTITION BY msno ORDER BY transaction_date  ), 'yyyyMMdd')),from_unixtime(unix_timestamp(membership_expire_date, 'yyyyMMdd'))) as sig_sub, \\\n",
    "                        date_sub(from_unixtime(unix_timestamp(transaction_date, 'yyyyMMdd')),0) as transaction_date_t, \\\n",
    "                        date_sub(from_unixtime(unix_timestamp(membership_expire_date, 'yyyyMMdd')),1) as membership_expire_date_t \\\n",
    "                from transactions t   order by msno, transaction_date\")\n",
    "\n",
    "susc_filtrada.write.format(\"parquet\").save(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/susc_filtrada\")\n",
    "susc_total.write.format(\"parquet\").save(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/susc_total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T19:11:12.940973Z",
     "start_time": "2018-04-14T19:11:12.151218Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "elementos = sqlContext.read.parquet(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/elementos\")\n",
    "elementos.createOrReplaceTempView(\"elementos\")\n",
    "susc_filtrada = sqlContext.read.parquet(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/susc_filtrada\")\n",
    "susc_filtrada.createOrReplaceTempView(\"susc_filtrada\")\n",
    "susc_total = sqlContext.read.parquet(\"gs://\"+str(RUTA_STORAGE)+\"input_tratado/susc_total\")\n",
    "susc_total.createOrReplaceTempView(\"susc_total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T00:45:51.066461Z",
     "start_time": "2018-04-15T00:44:53.265461Z"
    },
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21547746"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "susc_total_usada = sqlContext.sql(\"select msno,transaction_date,membership_expire_date,num_suscr,sig_sub, \\\n",
    "                       from_unixtime(unix_timestamp(transaction_date_t, 'yyyy-MM-dd')) as transaction_date_t, \\\n",
    "                       from_unixtime(unix_timestamp(membership_expire_date_t, 'yyyy-MM-dd')) as membership_expire_date_t   \\\n",
    "               from susc_total\")\n",
    "susc_total_usada.cache()\n",
    "susc_total_usada.createOrReplaceTempView(\"susc_total_usada\")\n",
    "sqlContext.sql(\"select * from  susc_total_usada\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T22:32:50.296585Z",
     "start_time": "2018-04-14T22:32:49.847007Z"
    },
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "446"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elemento_filtrado_actual = sqlContext.sql(\"select *, from_unixtime(unix_timestamp(date, 'yyyyMMdd')) as fecha_cast from elementos where msno = 'DwOmWms8HZUzNPPfiRwOMHj480D+GGqkDhNnSmZJm3s='  order by date desc\")\n",
    "elemento_filtrado_actual.cache()\n",
    "elemento_filtrado_actual.createOrReplaceTempView(\"elemento_filtrado_actual\")\n",
    "sqlContext.sql(\"select * from  elemento_filtrado_actual\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T01:23:41.949203Z",
     "start_time": "2018-04-15T01:23:41.922842Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "elemento_filtrado_actual = sqlContext.sql(\"select *, from_unixtime(unix_timestamp(date, 'yyyyMMdd')) as fecha_cast from elementos order by date desc\")\n",
    "elemento_filtrado_actual.cache()\n",
    "elemento_filtrado_actual.createOrReplaceTempView(\"elemento_filtrado_actual\")\n",
    "#sqlContext.sql(\"select * from  elemento_filtrado_actual\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-04-15T19:12:39.093Z"
    }
   },
   "source": [
    "## MODELO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T22:11:09.570012Z",
     "start_time": "2018-04-15T22:11:09.564971Z"
    }
   },
   "source": [
    "### Primer modelo SVMWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T22:15:44.874509Z",
     "start_time": "2018-04-15T22:11:25.068984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.10354648437743193\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "import array\n",
    "\n",
    "parsedData = datos_preparados.rdd.map(lambda row: LabeledPoint(row.label, row.features))\n",
    "model = SVMWithSGD.train(parsedData, iterations=100)\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(parsedData.count())\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T22:15:44.879783Z",
     "start_time": "2018-04-15T22:15:44.876819Z"
    }
   },
   "source": [
    "### Segundo modelo Regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T22:24:18.491127Z",
     "start_time": "2018-04-15T22:15:44.881594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.10354704024575606\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "parsedData = datos_preparados.rdd.map(lambda row: LabeledPoint(row.label, row.features))\n",
    "model = LogisticRegressionWithLBFGS.train(parsedData)\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(parsedData.count())\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T22:24:39.554051Z",
     "start_time": "2018-04-15T22:24:39.550563Z"
    }
   },
   "source": [
    "### Tercer modelo Regresión logística (Linear least squares, Lasso, and ridge regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T22:38:08.609455Z",
     "start_time": "2018-04-15T22:31:36.480626Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/mllib/regression.py:281: UserWarning: Deprecated in 2.0.0. Use ml.regression.LinearRegression.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use ml.regression.LinearRegression.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.9999998888263352\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD\n",
    "from numpy import array\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "from pyspark.mllib.feature import StandardScalerModel\n",
    "\n",
    "\n",
    "parsedData = datos_preparados.rdd.map(lambda row: LabeledPoint(row.label, row.features))\n",
    "model = LinearRegressionWithSGD.train(parsedData, iterations=100, step=0.00000001)\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(parsedData.count())\n",
    "print(\"Training Error = \" + str(trainErr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T22:24:18.524840Z",
     "start_time": "2018-04-15T22:19:11.346Z"
    }
   },
   "source": [
    "### Cuarto modelo Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T22:27:02.755063Z",
     "start_time": "2018-04-15T22:27:02.750578Z"
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.ml.classification import NaiveBayes\n",
    "#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "#\n",
    "#nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "#\n",
    "#parsedData = datos_preparados.rdd.map(lambda row: LabeledPoint(row.label, row.features))\n",
    "#\n",
    "#model = nb.fit(parsedData)\n",
    "#\n",
    "#predictions = model.transform(parsedData)\n",
    "#\n",
    "#predictions.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
